[gd_scene load_steps=3 format=3 uid="uid://b4b5fh23okx7s"]

[sub_resource type="SystemFont" id="SystemFont_fywoj"]
subpixel_positioning = 0

[sub_resource type="Theme" id="Theme_kitas"]
default_font = SubResource("SystemFont_fywoj")
default_font_size = 24

[node name="Help" type="Control"]
auto_translate_mode = 1
layout_mode = 3
anchors_preset = 15
anchor_right = 1.0
anchor_bottom = 1.0
grow_horizontal = 2
grow_vertical = 2

[node name="MarginContainer" type="MarginContainer" parent="."]
layout_mode = 1
anchors_preset = 15
anchor_right = 1.0
anchor_bottom = 1.0
grow_horizontal = 2
grow_vertical = 2
theme_override_constants/margin_left = 10
theme_override_constants/margin_top = 10
theme_override_constants/margin_right = 10
theme_override_constants/margin_bottom = 10

[node name="RichTextLabel" type="RichTextLabel" parent="MarginContainer"]
layout_mode = 2
focus_mode = 2
theme = SubResource("Theme_kitas")
text = "Local Agents runs llama.cpp-style chat fully offline inside Godot.

Runtime health checklist:
1. Build the extension in addons/local_agents/gdextensions/localagents.
2. Confirm platform binary exists under bin/ (macOS, Linux, or Windows).
3. Enable the Local Agents plugin and AgentManager autoload.
4. Open ChatExample and verify the sidebar shows Runtime: Ready.

Model download flow:
1. Open Local Agents bottom panel -> Downloads.
2. Download a GGUF model into user://local_agents/models.
3. Return to ChatExample and click Load Model.
4. Send prompts with Ctrl+Enter.

Tip: use Refresh Status in ChatExample after building or downloading."
selection_enabled = true
